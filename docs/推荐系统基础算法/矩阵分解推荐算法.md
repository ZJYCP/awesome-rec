# 矩阵分解推荐算法

---

> 矩阵分解(Matrix Factorization, MF)在协同过滤算法中"共现矩阵"的基础上，加入了隐向量的概念，加强了模型处理稀疏矩阵的能力。2006年，在Netflix举办的推荐算法竞赛Netflix Prize Challenge中，以矩阵分解为主的推荐算法大放异彩，拉开了矩阵分解在业界流行的序幕。



## 算法原理

矩阵分解算法是将用户评分矩阵$R$分解为用户矩阵$U$和物品矩阵$V$
$$
R_{m\times n} = U_{m\times k}V_{k\times n}
$$


我们称k维向量空间的每一维是隐因子(latent factor)，每个维度不具备与现实场景对应的具体可解释的含义，只 代表某种行为特性。矩阵分解的目的是通过机器学习的手段将用户行为矩阵中确实的数据填补完整，最终达到可以为用户做推荐的目的。



我们可以将矩阵分解转化为如下等价的求最小值的最优化问题：
$$
min_{p^\star,q^\star}\sum_{(u,v)\in A}(r_{uv} - p_u q_v^\top)^2+\lambda(||p_u||^2+||q_v||^2)
$$
加入正则化项，使得：

- 当模型参数过大，而输入数据发生变化时，可能会造成输出的不稳定。
- $L2$正则项等价于假设模型参数符合0均值的正态分布，从而使得模型的输出更加稳定。

## 求解方法



### 利用SGD来求解矩阵分解

具体可参考一般的随机梯度下降

### 利用ALS来求解矩阵分解

ALS: 交替最小二乘法

目前Spark MLlib中的协同过滤算法就是基于ALS求解的矩阵分解算法。在求解过程中，先固定$p_u$，目标函数就变成一个关于$q_v$的二次函数，可以作为最小二乘问题来求解，求出最优$q_v^\star$后，固定$q_v^\star$，再求解关于$p_u^$的最小问题，交替直到收敛。

相比SGD，ALS有如下优点：

1. 可以并行处理

2. 对于隐式特征问题比较合适

   TODO：待详细补充



## 扩展与优化

### 整合偏差项

主要由于不同的人的评价倾向可能不同，有人偏向评价高分，有人则不同。此外，标的物也会受到其他信息的干扰，比如由于主演的热点事件导致某视频突然火爆。

我们在之前的预测函数上增加偏差项，将评分表中的值分解为：全局均值，标的物偏差项，用户偏差，用户标的物交叉项。
$$
\hat{r}_{uv} = \mu + b_v + b_u + p_uq_v^\top
$$
优化函数也同样发生相应的改变。

### 增加更多的用户信息输入

通过引入跟多的信息来缓解用户评分过少的问题。具体而言，可以整合隐式反馈信息和用户人口统计信息。

对于隐式反馈信息，$I(u)$表示用户有过隐式反馈的标的物集合，$x_v \in \mathbb{R}^k$是标的物v的隐式反馈的特征向量,用户的对所有隐式反馈的累计贡献为：
$$
\sum_{v\in I(u)}x_v
$$
可以进一步进行归一化处理：
$$
|I_{u}|^{-0.5}\times \sum_{v\in I(u)}x_v
$$
同样的，对于人口统计学属性，S(u)表示用户的所有人口统计学信息，
$$
\sum_{a\in S(u)}y_a
$$
则用户预测公式变为：
$$
\hat{r}_{uv} = \mu + b_v + b_u + (p_u + |I_{u}|^{-0.5}\times \sum_{v\in I(u)}x_v + \sum_{a\in S(u)}y_a)q_v^\top
$$
优化目标函数也同样发生变化，太长了，就不写了。

### 整合时间因素

$$
\hat{r}_{uv}(t) = \mu + b_v(t) + b_u(t) + p_u(t)q_v^\top
$$

$b_v(t)$表示物品随时间的偏差，$b_u(t)$表示用户偏差随着时间的变化，$p_u(t)$表示用户的偏好变化。

### 整合用户对评分的置信度

一般来说，用户对不同标的物的评分不是完全可信的，用户对该标的物操作次数越多，时间越长，付出越大，相应的置信度也越大。我们可以在标的物的评分中增加一个置信度因子$c_{uv}$

### 隐式反馈

隐式反馈的矩阵分解

$r_{uv}$表示用户u对标的物v的隐式反馈，$p_{uv}$表示用户的对标的物的偏好
$$
p_{uv}=
$$


### 整合用户和标的物metadata信息



## 近实时矩阵分解算法







## 应用场景



## 优缺点

